# Voc√™ sabe a diferen√ßa entre Word Embedding, Chunking, Parsing e Tokenization?

## üî• Introdu√ß√£o

O universo da Intelig√™ncia Artificial, assim como em qualquer ramo da tecnologia, √© cheio de termos, m√©todos e t√©cnicas quem nem sempre s√£o f√°ceis de se entender e diferenciar. √Äs vezes os ouvimos e at√© repetimos, mas sem compreender bem do que estamos falando.

Aqui temos tr√™s deles, muito aplicados aos contextos de Agentic AI e arquiteturas como RAG. Mas voc√™ entende o que s√£o e sabe diferenci√°-los? Neste artigo vamos conceituar e esclarecer estes pontos, bem como falar da import√¢ncia e aplicabilidade de cada um deles.

Todos esses conceitos fazem parta da etapa de pr√© processamento de dados para NLP/PLN (Processamento de Linguagem Natural). S√£o t√©cnicas anteriores a populariza√ß√£o de Agentic AI, por√©m ganharam mais for√ßa devido seu papel neste contexto que est√° em alta.

Vamos abordar cada termo iniciando por sua defini√ß√£o te√≥rica, usando cita√ß√µes da literatura especializada. Ap√≥s, vamos discorrer sobre os eles em uma linguagem mais acess√≠vel, a fim de facilitar o entendimento.
  

## üîç Elementos lingu√≠sticos envolvidos

Para compreender bem os conceitos que s√£o os alvos deste artigo, vamos revisar alguns elementos lingu√≠sticos importantes, tais como os Grupos Gramaticais e as Sintagmas. Ambos s√£o utilizados para descrever diferentes aspectos da estrutura das frases e das palavras dentro de uma lingua.

### üóÇÔ∏è Grupos Gramaticais

Tamb√©m chamados "Classes Gramaticais" ou "Classes de Palavras", s√£o conjuntos de palavras que compartilham uma mesma fun√ß√£o sint√°tica ou morfol√≥gica dentro de uma frase. Exemplos de Grupos Gramaticais s√£o substantivos, verbos, adjetivos, pronomes, artigos, numerais, adv√©rbios, preposi√ß√µes, conjun√ß√µes e interjei√ß√µes, entre outros. Ou seja, as palavras s√£o organizadas em Grupos Gramaticais com base em seu papel e em sua estrutura na linguagem.

Cada Grupo Gramatical exerce uma fun√ß√£o espec√≠fica como, por exemplo, nomear objetos, indicar a√ß√µes, modificar significados, conectar ideias ou expressar sentimentos.‚Äã‚Äã Em resumo, o Grupo Gramatical t√™m o prop√≥sito de agrupar todas as palavras de um mesmo tipo morfol√≥gico e funcional dentro de um idioma.

Em suma, sintagmas s√£o grupos de elementos cont√≠guos que funcionam juntos na constru√ß√£o de frases, estruturando o sentido e a gramm√°tica do enunciado.

### üî† Sintagmas

Sintagma √© uma unidade lingu√≠stica formada por uma ou mais palavras. As palavras s√£o organizadas em torno de um n√∫cleo e desempenham fun√ß√£o sint√°tica dentro da frase. O Sintagma, a depender do n√∫cleo, pode ser verbal, nominal, adjetival, adverbial ou preposicional. As Sintagmas funcionam em conjunto para a constru√ß√£o das frases, estruturando o sentido e a gram√°tica.‚Äã

O tipo de Sintagma, como dito, √© definido pelo n√∫cleo. Logo, um Sintagma nominal √© um grupo de palavras cujo n√∫cleo √© um nome (substantivo). Uma Sintagm verbal t√™m como n√∫cleo um verbo. E assim por diante.

Considere como exemplo a frase ‚ÄúOs alunos chegaram agora‚Äù. Nela, ‚Äúos alunos‚Äù √© um sintagma nominal e ‚Äúchegaram agora‚Äù √© um sintagma verbal.


## üß© Tokenization (Tokeniza√ß√£o)

### ‚öôÔ∏è Conceito e utilidade

√â uma **etapa inicial do pr√©-processamento de textos** em linguagem natural (NLP) que faz a segmenta√ß√£o do texto. Em outras palavras, consiste em quebrar uma sequ√™ncia de texto em unidades menores. Nesta etapa, o texto √© dividido em **palavras**, **s√≠mbolos** ou **subpalavras**.

Jurafsky & Martin (2020), al√©m de definir o conceito de Tokenization, d√° tamb√©m uma ideia da sua aplicabilidade, conforme abaixo:

> ...etapa inicial que divide o texto em unidades menores (tokens), como palavras, subpalavras ou caracteres, viabilizando an√°lise automatizada e processamento eficiente de linguagem natural (Jurafsky & Martin, 2020).

Manning, Raghavan & Sch√ºtze (2008), em sua defini√ß√£o, abordam tamb√©m um pouco do funcionamento da tecnica:

> Na tokeniza√ß√£o, pegamos uma entrada (uma string) e um tipo de token (uma unidade significativa de texto, como uma palavra) e dividimos a entrada em partes (tokens) que correspondem ao tipo.

Considerando estas defini√ß√µes, podemos concluir que, no processo b√°sico de tokeniza√ß√£o por palavras, cada palavra corresponder√° a um token. No exemplo "O rato roeu a roupa do rei de Roma", ap√≥s o processo de tokeniza√ß√£o, teremos o vertor de tokens abaixo:

```
["O", "rato", "roeu", "a", "roupa", "do", "rei", "de", "Roma"]
```

√â mais f√°cil processar e analisar vetores do que longs testos em linguagen natural.

### üìå Tokenization em modelos LLM

Nos contextos de Agentic AI e Generative AI, que envolvem o uso de LLMs (modelos de linguagem de grande porte), a tokeniza√ß√£o funciona de modo diferente. Nesse caso, o conceito de "token" √© mais flex√≠vel que o de "palavra", a quantidade de tokens de um texto vai depender do m√©todo de tokeniza√ß√£o utilizado. Uma palavra pode ser representada por um ou mais tokens.

Em algoritmos como Byte-Pair Encoding (BPE) ou WordPiece palavras comuns ou curtas podem virar um token √∫nico, mas palavras longas, compostas, desconhecidas ou derivadas frequentemente s√£o divididas em v√°rios tokens menores, que podem ser subpalavras ou at√© caracteres individuais. Por exemplo, a palavra "darkness" pode ser dividida em dois tokens: ‚Äúdark‚Äù e ‚Äúness‚Äù.

Em muitos casos, contudo, √© interessante calcular quantos tokens est√£o sendo usados para, por exemplo, acionar modelos de LLM. Afinal, essa quantidade de tokens influencia no desempenho do modelo, em problemas como de alucina√ß√£o e no valor da fatura a ser paga para os provedores de servi√ßos em nuvem ou de modelos de ML.

### üíª Exemplo de implementa√ß√£o para contagem de tokens

Caso voc√™ queira fazer uma medi√ß√£o b√°sica, segue abaixo um c√≥digo de refer√™ncia. Isso pode ser √∫til para, por exemplo, estabelecer limites de consumo por seguran√ßa, gerar m√©tricas de consumo para trabalhos anal√≠ticos ou fazer proje√ß√£o de custos.

```
exemplo de implementa√ß√£o
```

## üî§ Parsing

### üî• O que √© Parsing?

Parsing √© a tarefa de analisar um conjunto sequencial de s√≠mbolos, tokens ou palavras, a fim de determinar a estrutura gramatical e/ou compreender a rela√ß√£o entre os elementos do conjunto.

No contexto da PNL, o parsing √© usado para interpretar senten√ßas segundo a gram√°tica da l√≠ngua em que essas senten√ßas se encontram. O Parsing √© capaz de identificar elementos gramaticais, tais como sujeito, verbo e predicado.

O parsing √© aplic√°vel a diversos contextos. Por exemplo, pode-se utilizar o parsing para analisar queries SQL. Pode-se utilizar o parsing para a an√°lise sint√°tica da query ou para um algor√≠timo com objetivo de otimiza-la, entre outras aplica√ß√µes poss√≠veis.

O termo "Parsing" √© usado para identificar tanto o processo de "parsing" quanto os algoritmos que implementam este processo.

### üéØ Objetivo do Parsing

O objetivo do Parsing √© transformar cadeias de texto em representa√ß√µes estruturadas, de acordo com a gram√°tica da lingua ou linguagem em que se encontram. Essa √© uma etapa fundamental para realizar tarefas como tradu√ß√£o autom√°tica, an√°lise sint√°tica e compreens√£o de textos e outros conjuntos de dados.

### üè∑Ô∏è Tipos de Parsing

Existem tipos diferentes de parsing, com objeticos espec√≠ficos necess√°rios a tarefa de interpreta√ß√£o de uma senten√ßa, conforme abaixo:

- **Parsing Sint√°tico:** Focado na estrutura da senten√ßa;
- **Parsing Sem√¢ntico:** Focado em encontrar o significado das rela√ß√µes entre os elementos da senten√ßa;
- **Parsing L√©xico:** Analisa menores unidades de entrada, que s√£o os tokens.

### üî¨ Parsing completo e parcial (chunking)

Os conceitos de parsing completo e parcial s√£o bastante discutidos no contexto e na literatura sobre PLN desde a d√©cada de 1980. Contudo, foi na obra entitulada ‚ÄúParsing by Chunks‚Äù, de 1992, de Steven P. Abney, que a ideia de parsing parcial foi formalizada sob o nome de "chunking".

O chunking (parsing parcial) √© uma alternativa ao parsing sint√°tico completo que influenciou de forma significativa o desenvolvimento de m√©todos r√°pidos e pr√°ticos em PLN.

O **parsing completo** (ou "full parsing") √© a **an√°lise detalhada da estrutura sint√°tica** de uma senten√ßa. Ele produz um mapeamento sint√°tico detalhado que mostra como cada parte da frase se relaciona com as demais.‚Äã

J√° o **parsing parcial** (o "chunking", tamb√©m chamado "shallow parsing") se refere a identifica√ß√£o dos **principais sintagmas**, tais como grupos nominais e verbais, sem detalhar hierarquias internas.‚Äã Obviamente, essa abordagem √© mais r√°pida. Ela √© √∫til para tarefas onde √© necess√°rio representar toda a estrutura sint√°tica da senten√ßa.‚Äã


## üî° Chunking

### üî• O que √© Chunking?

Chunking, como citado anteriormente, √© o termo definido na obra ‚ÄúParsing by Chunks‚Äù, de Abney (1992). O Chunking √© uma dentre outras formas dispon√≠veis para se implementar a t√©cnica do parcing parcial.

Logo, o Chunking √© a t√©cnica que consiste em agrupar tokens em blocos gramaticais, tais como Sintagmas Nominais ou Sintagmas Verbais. Um boa defini√ß√£o de Chunking, que cita os conceitos de parsing completo e parcial, √© a seguinte:

> No uso geral, os termos parsing e parser acabaram sendo adotados para se referir ao parsing completo. J√° o parsing parcial √© conhecido como chunking e a ferramenta como chunker, embora chunking seja uma dentre v√°rias abordagens para a implementa√ß√£o do parsing parcial (...). O conceito de chunk foi proposto por Abney (1992) como uma unidade formada por uma √∫nica palavra ou por um conjunto de palavras. Em um chunk, h√° uma palavra de conte√∫do circundada por palavras funcionais (Jurafsky & Martin, 2023). 

### üéØ Objetivo do Chunking

Seu objetivo √© faciliar a an√°lise sint√°tica e realizar extra√ß√£o de informa√ß√£o, conforme cita√ß√£o abaixo:

> M√©todo que segmenta textos em partes ou "chunks" maiores que palavras (ex.: frases, sintagmas), facilitando an√°lise sint√°tica e tarefas como extra√ß√£o de informa√ß√£o (Ramshaw & Marcus, 1995).

### Alternativas ao Chunking

Alguns dos m√©todos mais usados para **parsing parcial** s√£o baseados em t√©cnicas que combinam **regras lingu√≠sticas** e **modelos estat√≠sticos ou de aprendizado de m√°quina**. A seguir um resumo com alguns exemplos:

1. **Regras baseadas em gram√°tica regular:** Usam **express√µes regulares** ou **gram√°ticas de depend√™ncia superficial** para identificar padr√µes de sintagmas nominais (NP), verbais (VP) e preposicionais (PP). Pode ser implementada usando a bibliotecas **NLTK** (classe `RegexpParser`).

2. **Hidden Markov Models (HMMs)**: Modelo Estat√≠stico que trata o chunking como um problema de rotulagem sequencial, estimando a sequ√™ncia mais prov√°vel de chunks dados os tokens e suas etiquetas.

3. **Maximum Entropy Models (MEMMs):** Modelo Estat√≠stico 

4. **Conditional Random Fields (CRFs)**: Modelo Estat√≠stico ampliam os HMMs considerando depend√™ncias condicionais e recursos contextuais para maior precis√£o. S√£o usados em tarefas como o *CONLL-2000 shared task*, refer√™ncia cl√°ssica para chunking supervisionado.[2]

5. **Perceptrons e Support Vector Machines (SVMs):** treinados sobre corpora anotados (como o CoNLL corpus), eficazes para identificar fronteiras e tipos de chunks (NP, VP, PP).[3]

6. **Modelos baseados em Redes Neurais (como LSTMs ou BiLSTMs):** aplicam arquiteturas sequenciais profundas que consideram o contexto inteiro da frase para segmenta√ß√£o de chunks de forma mais robusta.[4]

7. **M√©todos Probabil√≠sticos e H√≠bridos:** Combinam **heur√≠sticas lingu√≠sticas** (gram√°ticas superficiais) com **probabilidade estat√≠stica** sobre corpora anotados, buscando equilibrar velocidade e precis√£o ‚Äî pr√°tica comum em sistemas atuais de PLN em larga escala.[5][3]

Em s√≠ntese, os m√©todos mais proeminentes atualmente s√£o os **Modelos de Markov Ocultos (HMMs)**, **Conditional Random Fields (CRFs)** e as **redes neurais recorrentes (RNNs/BiLSTMs)**, frequentemente combinados com pr√©-processamentos gramaticais e POS tagging. Esses m√©todos assumiram a lideran√ßa devido √† sua **efici√™ncia computacional** e **precis√£o em identificar constituintes superficiais** de frases complexas.


## üî¢ Word Embedding

Depois que o texto √© tokenizado, cada token (palavra, subpalavra etc.) √© **convertido em vetor num√©rico cont√≠nuo**. Esses vetores permitem que o modelo ‚Äúentenda‚Äù semelhan√ßas sem√¢nticas e sint√°ticas entre as palavras.

A ideia √© transformar palavras, que s√£o entidades simb√≥licas, em n√∫meros que um computador consiga manipular de forma eficiente, preservando **rela√ß√µes sem√¢nticas e sint√°ticas**.

### Caracter√≠sticas principais:

* Cada palavra √© representada como um **vetor em um espa√ßo de alta dimens√£o** (por exemplo, 50, 100, 300 dimens√µes).
* Palavras com significados semelhantes ficam pr√≥ximas nesse espa√ßo vetorial.
  üëâ Exemplo: os vetores de **"rei"** e **"rainha"** ficam mais pr√≥ximos que **"rei"** e **"banana"**.
* Captura n√£o s√≥ a identidade da palavra, mas tamb√©m rela√ß√µes como:
  [
  \text{vetor("rei")} - \text{vetor("homem")} + \text{vetor("mulher")} \approx \text{vetor("rainha")}
  ]

### Principais modelos de Word Embeddings:

* **Word2Vec (Google, 2013):** usa redes neurais simples (CBOW e Skip-Gram) para aprender embeddings.
* **GloVe (Stanford, 2014):** baseado em estat√≠sticas globais de coocorr√™ncia de palavras.
* **FastText (Facebook, 2016):** leva em conta subpalavras (prefixos/sufixos), ajudando com palavras raras ou novas.

### Por que √© √∫til?

Antes, as palavras eram representadas por **one-hot encoding** (vetores esparsos e de alta dimens√£o, sem sem√¢ntica).
Os embeddings resolvem esse problema ao criar representa√ß√µes densas, permitindo que algoritmos de ML e Deep Learning entendam melhor o **contexto e o significado**.



## üéØ Conclus√£o

* **Tokeniza√ß√£o** = dividir.
* **Chunking** = agrupar em peda√ßos lingu√≠sticos (chuncks).

Essas etapas s√£o **pr√©-processamento** do texto.

Word Embedding √© uma **t√©cnica de representa√ß√£o de palavras em forma de vetores num√©ricos cont√≠nuos**, usada em **Processamento de Linguagem Natural (PLN / NLP)**.


| Etapa           | O que faz                                                 | Exemplo                                       |
|:----------------|:----------------------------------------------------------|:--------------------------------------------- |
| **Tokeniza√ß√£o** | Divide o texto em palavras, s√≠mbolos ou subpalavras       | "O gato dorme." ‚Üí ["O", "gato", "dorme", "."] |
| **Chunking**    | Agrupa tokens em blocos gramaticais                       | `"O gato dorme"` ‚Üí `[NP O gato] [VP dorme]`   |
| **Embedding**   | Transforma cada token em um vetor que captura significado | `"gato"` ‚Üí `[0.12, -0.85, 0.33, ...]`         |


**exemplo pr√°tico em Python** mostrando claramente a diferen√ßa entre **tokeniza√ß√£o**, **chunking** e **word embeddings**, usando a biblioteca **spaCy** (uma das mais populares em NLP).

---

### üíª Exemplo completo com `spaCy`

```python
# Instale o modelo de linguagem do spaCy (apenas uma vez):
# !python -m spacy download pt_core_news_md

import spacy

# Carrega o modelo pr√©-treinado em portugu√™s (com embeddings)
nlp = spacy.load("pt_core_news_md")

# Texto de exemplo
texto = "O gato dorme no sof√° enquanto o cachorro observa."

# Processa o texto
doc = nlp(texto)

# --- 1Ô∏è‚É£ TOKENIZA√á√ÉO ---
print("=== TOKENIZA√á√ÉO ===")
for token in doc:
    print(token.text)

# --- 2Ô∏è‚É£ CHUNKING (agrupamento em sintagmas nominais) ---
print("\n=== CHUNKS (Sintagmas Nominais) ===")
for chunk in doc.noun_chunks:
    print(chunk.text)

# --- 3Ô∏è‚É£ WORD EMBEDDING ---
print("\n=== EMBEDDINGS (vetores num√©ricos) ===")
token_exemplo = doc[1]  # palavra "gato"
print(f"Palavra: {token_exemplo.text}")
print(f"Vetor (primeiros 10 valores): {token_exemplo.vector[:10]}")
print(f"Tamanho do vetor: {len(token_exemplo.vector)} dimens√µes")

# --- 4Ô∏è‚É£ SIMILARIDADE SEM√ÇNTICA ---
print("\n=== SIMILARIDADE ENTRE PALAVRAS ===")
gato = nlp("gato")[0]
cachorro = nlp("cachorro")[0]
sofa = nlp("sof√°")[0]

print(f"similaridade(gato, cachorro) = {gato.similarity(cachorro):.3f}")
print(f"similaridade(gato, sof√°) = {gato.similarity(sof√°):.3f}")
```

---


### üß™ Sa√≠da esperada (resumo):

```
=== TOKENIZA√á√ÉO ===
O
gato
dorme
no
sof√°
enquanto
o
cachorro
observa
.

=== CHUNKS (Sintagmas Nominais) ===
O gato
O cachorro

=== EMBEDDINGS (vetores num√©ricos) ===
Palavra: gato
Vetor (primeiros 10 valores): [ 0.12, -0.32, 0.45, ...]
Tamanho do vetor: 300 dimens√µes

=== SIMILARIDADE ENTRE PALAVRAS ===
similaridade(gato, cachorro) = 0.78
similaridade(gato, sof√°) = 0.32
```


## üìö Bibliografia

ABNEY, Steven P. Parsing by Chunks. In: PRINCIPLE-BASED PARSING. Dordrecht: Springer, 1992. p. 257-278.

ABNEY, Steven P.; TENNY, Carol. Parsing by Chunks. In: Principle-Based Parsing. Dordrecht: Springer, 1992. p. 257-278.

BLOG RACKSPACE. Chunking NLP Techniques. 2024. Dispon√≠vel em: <https://www.rackspace.com/blog/how-chunking-strategies-work-nlp>. Acesso em: 25 out. 2025.

BRASILEIRAESPLN. Cap√≠tulo 7 Ferramentas e recursos para o processamento sint√°tico. Dispon√≠vel em: <https://brasileiraspln.com/livro-pln/1a-edicao/parte4/cap7/cap7.html>. Acesso em: 25 out. 2025.

CAMACHO-COLLADOS, Jose; PILEHVAR, Mohammad Taher. Embeddings in Natural Language Processing. Draft. Dispon√≠vel em: <https://josecamachocollados.com/book_embNLP_draft.pdf>. Acesso em: 25 out. 2025.

DATACAMP. O que √© tokeniza√ß√£o? Tipos, casos de uso e implementa√ß√£o. 2024. Dispon√≠vel em: <https://www.datacamp.com/pt/blog/what-is-tokenization>. Acesso em: 25 out. 2025.

GOLDBERG, Yoav. Neural Network Methods for Natural Language Processing. San Rafael, CA: Morgan & Claypool, 2017.

JURAFSKY, Daniel; MARTIN, James H. Speech and Language Processing. 3. ed. draft (2023). Stanford University. Dispon√≠vel em: <https://web.stanford.edu/~jurafsky/slp3/>. Acesso em: 25 out. 2025.

JURAFSKY, Daniel; MARTIN, James H. Speech and Language Processing. 3. ed. Stanford: Stanford University, 2023. Dispon√≠vel em: <https://web.stanford.edu/~jurafsky/slp3/2.pdf>. Acesso em: 25 out. 2025.

JURAFSKY, Daniel; MARTIN, James H. Speech and Language Processing. 3. ed. preliminar. Stanford: Stanford University, 2020. Dispon√≠vel em: <https://www.datacamp.com/pt/blog/what-is-tokenization>. Acesso em: 25 out. 2025.

MANNING, Christopher D.; RAGHAVAN, Prabhakar; SCH√úTZE, Hinrich. Introduction to Information Retrieval. Cambridge: Cambridge University Press, 2008. Dispon√≠vel em: <https://smltar.com/tokenization.html>. Acesso em: 25 out. 2025.

MANNING, Christopher D.; SCH√úTZE, Hinrich. Foundations of Statistical Natural Language Processing. Cambridge: MIT Press, 1999.

MICHAELIS Dicion√°rio. Sintagma. Dispon√≠vel em: <https://michaelis.uol.com.br/moderno-portugues/busca/portugues-brasileiro/sintagma>. Acesso em: 25 out. 2025.

MIKOLOV, Tomas; CHEN, Kai; CORRADO, Greg; DEAN, Jeffrey. Efficient estimation of word representations in vector space. arXiv, 2013. Dispon√≠vel em: <https://www.ibm.com/br-pt/think/topics/word-embeddings>. Acesso em: 25 out. 2025.

NVIDIA BLOG. Explicando Tokens: A Linguagem e a Moeda da IA. 2025. Dispon√≠vel em: <https://blog.nvidia.com.br/blog/tokens-ia-explicados/>. Acesso em: 25 out. 2025.

RAMSHAW, Lance A.; MARCUS, Mitchell P. Text chunking using transformation-based learning. In: Very Large Corpora, 3., 1995, Cambridge. Proceedings... Cambridge: Cambridge University Press, 1995. Dispon√≠vel em: <https://www.rackspace.com/blog/how-chunking-strategies-work-nlp>. Acesso em: 25 out. 2025.

TODAMATERIA. Sintagma nominal e verbal explicados com exemplos. 2022. Dispon√≠vel em: <https://www.todamateria.com.br/sintagma-nominal-e-verbal/>. Acesso em: 25 out. 2025.
